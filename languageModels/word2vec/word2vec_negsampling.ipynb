{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling (Skipgram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Word2VecDataset:\n",
    "    def __init__(self, corpus, window_size=2):\n",
    "        self.corpus = corpus\n",
    "        self.window_size = window_size\n",
    "        self.word_to_idx, self.idx_to_word, self.vocab_size = self.build_vocab()\n",
    "        self.data = self.generate_training_pairs()\n",
    "\n",
    "    def build_vocab(self):\n",
    "        words = [word for sentence in self.corpus for word in sentence]\n",
    "        word_counts = Counter(words)\n",
    "        vocab = list(word_counts.keys())\n",
    "        word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "        return word_to_idx, idx_to_word, len(vocab)\n",
    "\n",
    "    def generate_training_pairs(self):\n",
    "        pairs = []\n",
    "        for sentence in self.corpus:\n",
    "            indices = [self.word_to_idx[word] for word in sentence]\n",
    "            for center_pos, center_idx in enumerate(indices):\n",
    "                for offset in range(-self.window_size, self.window_size + 1):\n",
    "                    context_pos = center_pos + offset\n",
    "                    if context_pos >= 0 and context_pos < len(indices) and context_pos != center_pos:\n",
    "                        pairs.append((center_idx, indices[context_pos]))\n",
    "        return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self, vocab_size, num_negative_samples=5):\n",
    "        super(NegativeSamplingLoss, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "\n",
    "    def forward(self, center_embeds, target_indices, model):\n",
    "        \n",
    "        true_logits = torch.matmul(center_embeds, model.outside_embeddings(target_indices).T)\n",
    "        positive_loss = -torch.log(torch.sigmoid(true_logits))\n",
    "\n",
    "        # Negative Sample Loss\n",
    "        neg_samples = torch.randint(0, self.vocab_size, (center_embeds.size(0), self.num_negative_samples))\n",
    "        negative_embeds = model.outside_embeddings(neg_samples)\n",
    "        negative_logits = torch.bmm(negative_embeds, center_embeds.unsqueeze(2)).squeeze(2)\n",
    "        negative_loss = -torch.sum(torch.log(torch.sigmoid(-negative_logits)), dim=1)\n",
    "\n",
    "       \n",
    "        total_loss = torch.sum(positive_loss + negative_loss)\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self , vocab_size , embedding_dim):\n",
    "        super(Word2Vec , self).__init__()\n",
    "        self.center_embedding = nn.Embedding(vocab_size , embedding_dim)\n",
    "        self.outside_embedding = nn.Embedding(self.vocab_size , embedding_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self , center_words):\n",
    "        center_embeds  = self.center_embedding(center_words)\n",
    "        return center_embeds \n",
    "    \n",
    "    \n",
    "    def predict(self , center_embeds):\n",
    "        logits = torch.matmul(center_embeds , self.outside_embedding.T)\n",
    "        probs = torch.softmax(logits , dim = 1 )\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 3: Training Loop for Negative Sampling\n",
    "def train_word2vec_negative_sampling(dataset, embedding_dim=10, epochs=10, learning_rate=0.01, num_negative_samples=5):\n",
    "    vocab_size = dataset.vocab_size\n",
    "    model = Word2Vec(vocab_size, embedding_dim)\n",
    "    loss_fn = NegativeSamplingLoss(vocab_size, num_negative_samples)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for center_idx, outside_idx in dataset.data:\n",
    "            # Convert to tensors\n",
    "            center_tensor = torch.tensor([center_idx], dtype=torch.long)\n",
    "            outside_tensor = torch.tensor([outside_idx], dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            center_embeds = model(center_tensor)\n",
    "            loss = loss_fn(center_embeds, outside_tensor, model)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    [\"I\", \"like\", \"learning\", \"deep\", \"learning\"],\n",
    "    [\"deep\", \"learning\", \"is\", \"fun\"],\n",
    "    [\"word2vec\", \"uses\", \"word\", \"embeddings\"]\n",
    "]\n",
    "Dataset = Word2VecDataset(corpus)\n",
    "\n",
    "\n",
    "# Example Usage for Negative Sampling\n",
    "trained_model_negative_sampling = train_word2vec_negative_sampling(Dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
