# ğŸ§  Foundation Models for Vision, Language & Action

<p align="center">
  <img src="main.png" alt="Foundation Models Header" width="600"/>
  <br>
<sub><strong><em>Made by AI with Compute</em></strong></sub>
</p>

---

## 1ï¸. Vision Models

### âœ”ï¸ Papers Done 
- **CLIP** â€“ [Contrastive Languageâ€“Image Pretraining (2021)](https://arxiv.org/abs/2103.00020)  
- **ViT** â€“ [An Image is Worth 16x16 Words (2020)](https://arxiv.org/abs/2010.11929)  
- **AlexNet** â€“ [ImageNet Classification with Deep Convolutional Neural Networks (2012)](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

---

## 2ï¸. Language Models

### âœ”ï¸ Papers Done
- **LLaMA 2** â€“ [LLaMA 2: Open Foundation and Fine-Tuned Chat Models (2023)](https://arxiv.org/abs/2307.09288)
- **Mistral MoE** â€“ [Mixtral of Experts (2023)](https://arxiv.org/abs/2401.04088)
- **GPT-2** â€“ [Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

---

## 3ï¸. Unsupervised Learning

### âœ”ï¸ Papers Done
- **VAE (MLP)** â€“ Variational Autoencoder using fully-connected networks
- **VAE (U-Net CNN)** â€“ Convolutional VAE with U-Net-style encoder/decoder 

--- 

## 4ï¸. Action Models 

### To Be Updated
- *(Work in progress)*
