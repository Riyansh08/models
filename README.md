# 🧠 Foundation Models for Vision, Language & Action

<p align="center">
  <img src="main.png" alt="Foundation Models Header" width="600"/>
  <br>
<sub><strong><em>Made by AI with Compute</em></strong></sub
</p>

---

## 1️⃣ Vision Models

### ✔️ Papers Done 
- **CLIP** – [Contrastive Language–Image Pretraining (2021)](https://arxiv.org/abs/2103.00020)
- **ViT** – [An Image is Worth 16x16 Words (2020)](https://arxiv.org/abs/2010.11929)

---

## 2️⃣ Language Models

### ✔️ Papers Done
- **Mistral MoE** – [Mixtral of Experts (2023)](https://arxiv.org/abs/2401.04088)
- **GPT-2** – [Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)


---

##  3️⃣  Unsupervised Learning

### ✔️ Papers Done
- **VAE (MLP)** – Variational Autoencoder using fully-connected networks
- **VAE (U-Net CNN)** – Convolutional VAE with U-Net-style encoder/decoder 

--- 

## 4️⃣ Action Models 

###  To Be Updated
- *(Work in progress)*

