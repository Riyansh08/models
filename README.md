# 🧠 Foundation Models for Vision, Language & Action

<p align="center">
  <img src="main.png" alt="Foundation Models Header" width="600"/>
  <br>
<sub><strong><em>Made by AI with Compute</em></strong></sub>
</p>

---

## 1️. Vision Models

### ✔️ Papers Done 
- **CLIP** – [Contrastive Language–Image Pretraining (2021)](https://arxiv.org/abs/2103.00020)  
- **ViT** – [An Image is Worth 16x16 Words (2020)](https://arxiv.org/abs/2010.11929)  
- **AlexNet** – [ImageNet Classification with Deep Convolutional Neural Networks (2012)](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

---

## 2️. Language Models

### ✔️ Papers Done
- **LLaMA 2** – [LLaMA 2: Open Foundation and Fine-Tuned Chat Models (2023)](https://arxiv.org/abs/2307.09288)
- **Mistral MoE** – [Mixtral of Experts (2023)](https://arxiv.org/abs/2401.04088)
- **GPT-2** – [Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

---

## 3️. Unsupervised Learning

### ✔️ Papers Done
- **VAE (MLP)** – Variational Autoencoder using fully-connected networks
- **VAE (U-Net CNN)** – Convolutional VAE with U-Net-style encoder/decoder 

--- 

## 4️. Action Models 

### To Be Updated
- *(Work in progress)*
