# ğŸ§  Foundation Models for Vision, Language & Action

<p align="center">
  <img src="main.png" alt="Foundation Models Header" width="600"/>
  <br>
<sub><strong><em>Made by AI with Compute</em></strong></sub>
</p>

---

## 1ï¸. Vision Models

### âœ”ï¸ Papers Done 
- **CLIP** â€“ [Contrastive Languageâ€“Image Pretraining (2021)](https://arxiv.org/abs/2103.00020)  
- **ViT** â€“ [An Image is Worth 16x16 Words (2020)](https://arxiv.org/abs/2010.11929)  
- **AlexNet** â€“ [ImageNet Classification with Deep Convolutional Neural Networks (2012)](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

---

## 2ï¸. Language Models

### âœ”ï¸ Papers Done
- **LLaMA 2** â€“ [LLaMA 2: Open Foundation and Fine-Tuned Chat Models (2023)](https://arxiv.org/abs/2307.09288)
- **Mistral MoE** â€“ [Mixtral of Experts (2023)](https://arxiv.org/abs/2401.04088)
- **GPT-2** â€“ [Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- **Word2Vec** â€“ [Word2Vec (2014)](https://arxiv.org/abs/1301.3812)
- **Gemma** â€“ [Gemma: A Generalized Multi-Modal Foundation Model (2024)](https://arxiv.org/abs/2401.04088)



---

## 3ï¸. Unsupervised Learning

### âœ”ï¸ Papers Done
- **VAE (MLP)** â€“ Variational Autoencoder using fully-connected networks
- **VAE (U-Net CNN)** â€“ Convolutional VAE with U-Net-style encoder/decoder 

--- 

## 4. RL 

### âœ”ï¸ Papers Done
- **PPO** â€“ [Proximal Policy Optimization (2017)](https://arxiv.org/abs/1707.06347)
- **DQN** â€“ [Deep Q Learning (2013)](https://arxiv.org/abs/1310.5670)

--- 

## 5. Action Models 

### To Be Updated
- *(Work in progress)*

---

## 6. Fine-Tuning 

###  âœ”ï¸ Papers Done
- **LoRA** â€“ [LoRA: Low-Rank Adaptation for Fast Fine-Tuning (2023)](https://arxiv.org/abs/2304.00062)